{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EYE FOR BLIND\nThis notebook will be used to prepare the capstone project 'Eye for Blind'","metadata":{"id":"pKB2TAPdC7uu"}},{"cell_type":"code","source":"#Import all the required libraries\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport glob\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport os\nimport zipfile\nfrom PIL import Image\nimport random\n\n# Date and Time \nimport datetime,time\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\nimport collections, random, re\nfrom collections import Counter\nimport operator\n\n# tensorflow , KERAS Libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array \nfrom tensorflow.keras.utils import plot_model\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom tqdm import tqdm\nfrom keras.utils.vis_utils import plot_model\n\n\n# Model building \nfrom sklearn.model_selection import train_test_split\n","metadata":{"id":"GUpAO6BIC7uy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## extarct the data if not present\nif os.path.isdir('/content/Flickr8K/'):\n  print(\"Flickr8K already present\")\nelse:\n  zf = zipfile.ZipFile ('/content/drive/MyDrive/Flickr8K.zip', 'r')\n  zf.extractall('/content/Flickr8K/')\n  zf.close()","metadata":{"id":"91WqH1b6E15-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's read the dataset","metadata":{"id":"UStnn9kZC7uz"}},{"cell_type":"markdown","source":"## Data understanding\n1.Import the dataset and read image & captions into two seperate variables\n\n2.Visualise both the images & text present in the dataset\n\n3.Create word-to-index and index-to-word mappings.\n\n4.Create a dataframe which summarizes the image, path & captions as a dataframe\n\n5.Visualise the top 30 occuring words in the captions\n\n6.Create a list which contains all the captions & path\n","metadata":{"id":"TYe4tymtC7uz"}},{"cell_type":"code","source":"## images and text path\nimages_path='/content/Flickr8K/Images'\n\ntext_path= '/content/Flickr8K/captions.txt'","metadata":{"id":"89_rPe7TKe0D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import the dataset and read the image into a seperate variable\nall_imgs = glob.glob(images_path + '/*.jpg',recursive=True)\nprint(\"The total images present in the dataset: {}\".format(len(all_imgs)))","metadata":{"id":"Divw6VZeC7u0","outputId":"363ff978-07c9-4f82-8d4b-3c75f326d42c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import the dataset and read the text file into a seperate variable\ndef load_doc(filename):\n    lines = []\n    with open(filename) as f:\n        lines = f.readlines()\n        f.close()\n    return lines","metadata":{"id":"JRg6u7CAC7u1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc = load_doc(text_path)\n\n## deleting the 1st line from the text as its the header\ndel doc[0]\n\nprint(doc[:3])","metadata":{"id":"eG1LFW5nKxl0","outputId":"aed4a57e-3369-45a4-a027-c92500b52d7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualise both the images & text present in the dataset\n\ndef Visualise_image_text(image):\n    imgPath_vis = all_imgs[0]\n    plt.figure(figsize=(6,4))\n    plt.imshow(mpimg.imread(imgPath_vis))\n\n    for i in range(len(doc)):\n        filename = os.path.basename(imgPath_vis)\n        if str(doc[i]).__contains__(filename):\n            print(doc[i][len(filename)+1:])","metadata":{"id":"Uc1VuD-WC7u1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting one Image & texts\nVisualise_image_text(all_imgs[0])","metadata":{"id":"L_40oHvUK6eJ","outputId":"8b45f113-433d-4112-b2b9-9771c697483d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a dataframe which summarizes the image, path & captions as a dataframe\n\nEach image id has 5 captions associated with it therefore the total dataset should have 40455 samples.","metadata":{"id":"A3z5sSomC7u2"}},{"cell_type":"code","source":"print(\"No of Images:\" , len(all_imgs))\nprint(\"No of Captions:\" , len(doc))","metadata":{"id":"Hf4Ah-ITC7u2","outputId":"039ec1af-aa12-47e8-e5b8-5bab41d568ff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe_file = \"dataframe_file.pkl\"","metadata":{"id":"1B9Na0cYB-Jp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### save and read the dataframe that stores image id , path and captions so that we don't have to rerun the model everytime.\n\ndef read_dataframe(filename):\n  ##if file is not extracted the extarct and use it\n  read_df = pd.read_pickle(filename)\n  return read_df\n\ndef save_dataframe(filename,dataframe):\n  dataframe.to_pickle(filename)","metadata":{"id":"QMngaay-CCAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_img_id= []#store all the image id here\nall_img_vector= []#store all the image path here\nannotations= [] #store all the captions here\n\n## if df file is present then read from there else create df file\nif os.path.isfile(dataframe_file):\n    df = read_dataframe(dataframe_file)\n    all_img_id= df.ID\n    all_img_vector= df.Path\n    annotations= df.Captions\nelse: ## create the dataframe and save it for future use\n    for i in range(len(all_imgs)):\n        fileName = os.path.basename(all_imgs[i])\n        filePath = all_imgs[i]\n        for j in range(len(doc)):\n            if str(doc[j]).__contains__(fileName):\n                annotations.append(str(doc[j][len(fileName)+1:]).strip())\n                all_img_id.append(fileName)\n                all_img_vector.append(filePath)                \n    df = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions'])\n    save_dataframe(dataframe_file,df)\n    \ndf.head(10)","metadata":{"id":"mJKulplXCF6f","outputId":"82e77a17-6a23-4c20-cfdf-a37ffd36e8b0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## sorting the dataframe based on image id\ndf.sort_values(by=\"ID\",inplace=True)","metadata":{"id":"7oRgeT8Bgjmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## checking the shape of the fianl dataframe\ndf.shape","metadata":{"id":"bJVmctwSX2vw","outputId":"684afae3-f024-4dc5-be5b-77f33fdab9b3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create the vocabulary & the counter for the captions\nfilter_chars = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\nvocabulary={} \n\nfor lines in df.Captions:\n  ## using keras api to convert lines to words\n  for word in text_to_word_sequence(lines,filters=filter_chars,lower=True, split=' '):\n      ## adding the word to the dict and the count  \n      if word not in vocabulary:\n        vocabulary[word] = 1\n      else:\n        vocabulary[word] = vocabulary[word] + 1\n\nval_count=len(vocabulary)\nval_count","metadata":{"id":"yk16VKJEC7u4","outputId":"1b41258a-34c9-4453-feb0-059515d275bd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## creating a dataframe for the words and count\ndf_word = pd.DataFrame.from_dict(vocabulary, orient='index')\ndf_word = df_word.sort_values(by=[0],ascending=False).reset_index()\ndf_word =df_word.rename(columns={'index':'word', 0:'count'})","metadata":{"id":"SSB_clrdlxzh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## function to Visualise words and count\ndef show_top_words(index,words,count):\n    plt.figure(figsize=(20,3))\n    plt.bar(words,count,color='maroon', width =0.4)\n    plt.xlabel(\"Words\",  fontsize=20) \n    plt.ylabel(\"Word Count\",rotation=90,fontsize=20) \n    plt.xticks(index,words,rotation=90,fontsize=20)\n    plt.title(\"The top \"+ str(len(index)) + \" most frequently appearing words\",fontsize=20)\n    plt.show()","metadata":{"id":"JfKyMbLZmB0t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualise the top 30 occuring words in the captions\nwords = list(df_word[0:30].word)\ncount =list(df_word['count'][0:30])\nshow_top_words(list(range(0,30)),words,count)","metadata":{"id":"dyoUk-7EC7u4","outputId":"cc8b9dbf-dcd7-4099-925c-8b565591a4c9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Add <start> and <end> tags in the words\ndf['Captions']=df.Captions.apply(lambda x : f\"<start> {x} <end>\")","metadata":{"id":"A6nkcwwypK3o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## disply the updated captions\ndf.head(5)","metadata":{"id":"Of64Y5mwqQjp","outputId":"4edf0d9e-17f1-43f9-e1e1-2dbffe8c7457"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a list which contains all the captions\nannotations=df['Captions']\n\n# Find max length of sequence excluding the spaces between words\nmax_length = max(df.Captions.apply(lambda x : len(x.split())))\n\n#Create a list which contains all the path to the images\nall_img_path = df.Path\nunique_img = df.Path.unique()\nprint(\"Total captions present in the dataset: \"+ str(len(annotations)))\nprint(\"Total images present in the dataset: \" + str(len(all_img_path)))\nprint(\"Total Unique images present in the dataset: \" + str(len(unique_img)))\nprint(\"Max words of a sentence is :\",max_length)","metadata":{"id":"AWUdyHqfC7u4","outputId":"6e3e222a-afd3-4acf-c88f-5a1a05273020"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## function to plot image and text side by side\ndef plot_image_captions(Pathlist,captionsList,fig,count=2,npix=299,nimg=2):\n        image_load = load_img(Path,target_size=(npix,npix,3))\n        ax = fig.add_subplot(nimg,2,count,xticks=[],yticks=[])\n        ax.imshow(image_load)\n        \n        count +=1\n        ax = fig.add_subplot(nimg,2,count)\n        plt.axis('off')\n        ax.plot()\n        ax.set_xlim(0,1)\n        ax.set_ylim(0,len(captions))\n        for i, caption in enumerate(captions):\n            ax.text(0,i,caption,fontsize=10)","metadata":{"id":"WfsE8uH5rVbr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Show 5 images and corresponding text side by side \nfig = plt.figure(figsize=(10,20))\ncount = 1\n    \nfor Path in df[:20].Path.unique():\n    captions = list(df[\"Captions\"].loc[df.Path== Path].values)\n    plot_image_captions(Path,captions,fig,count,299,5)\n    count +=2\nplt.show()","metadata":{"id":"7xDk1DoRrbW7","outputId":"4282757d-46fa-456b-e606-c2b93760383d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-Processing the captions\n1.Create the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters. \nThis gives us a vocabulary of all of the unique words in the data. Keep the total vocaublary to top 5,000 words for saving memory.\n\n2.Replace all other words with the unknown token \"UNK\" .\n\n3.Create word-to-index and index-to-word mappings.\n\n4.Pad all sequences to be the same length as the longest one.","metadata":{"id":"7_e4tVkiC7u5"}},{"cell_type":"code","source":"# create the tokenizer function\ndef tokenize_captions(top_freq_words,captions):\n    special_chars = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ '\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_freq_words,\n                                                  oov_token=\"UNK\",\n                                                  filters=special_chars,\n                                                  lower=True, split=' ', char_level=False)\n    tokenizer.fit_on_texts(captions)\n    \n    # Adding PAD to tokenizer list on index 0\n    tokenizer.word_index['PAD'] = 0\n    tokenizer.index_word[0] = 'PAD'   \n   \n    return tokenizer","metadata":{"id":"amabfjtIC7u5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the tokenized vectors\ntop_freq_words = 5000\ntokenizer = tokenize_captions(top_freq_words,df['Captions'])\ncap_seqs = tokenizer.texts_to_sequences(df['Captions'])","metadata":{"id":"M9cYuacPyS_n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cap_seqs[:5]","metadata":{"id":"zwaJ3792sya6","outputId":"4d426a6f-4f81-4265-950b-b80385c6e40a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create word-to-index and index-to-word mappings functions.\ndef show_word_2_index(word):\n    print(\"Word = {}, index = {}\".format(word, tokenizer.word_index[word]))\n\ndef show_index_2_word(index):\n    print(\"Index = {}, Word = {}\".format(index, tokenizer.index_word[index]))","metadata":{"id":"V4GtqsyHC7u5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### show word-to-index and index-to-word mappings \n          \nprint(\"------Word to Index------\")\nshow_word_2_index(\"PAD\")\nshow_word_2_index(\"UNK\")\nshow_word_2_index(\"<start>\")\nshow_word_2_index(\"<end>\")\n\nprint('\\n')\n\nprint(\"-------Index to Word--------\")\nshow_index_2_word(2)\nshow_index_2_word(1500)\nshow_index_2_word(3000)\nshow_index_2_word(4999)","metadata":{"id":"eacoUTJwys4D","outputId":"242e80c2-6cb5-41c7-8ac8-dced51551398"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a word count of your tokenizer to visulize the Top 30 occuring words after text processing\n\nword_count = pd.DataFrame.from_dict(tokenizer.word_counts,orient='index')\nword_count.sort_values(by=[0],ascending=False , inplace=True)\n\nwords = list(word_count[0:30].index)\ncount =list(word_count[0:30][0])\nshow_top_words(list(range(0,30)),words,count)","metadata":{"id":"EULOXH-4C7u6","outputId":"fd086bb7-eab3-4c57-9aa4-b21f5f242938"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pad each vector to the max_length of the captions & store it to a vairable\n\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(cap_seqs, padding='post')\n\nprint(\"The shape of Caption vector is :\" + str(cap_vector.shape))\nprint(cap_vector[:5])","metadata":{"id":"6JDj6bikC7u6","outputId":"4cf16cb7-2707-4068-ae71-1904440f2bba"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-processing the images\n\n1.Resize them into the shape of (299, 299)\n\n3.Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3. ","metadata":{"id":"UFu04BFsC7u6"}},{"cell_type":"code","source":"## as we will be using Inception V3 for trasfer learning for encoding , thats why using image size as below\nimage_shape = (299, 299)","metadata":{"id":"k74zUiU6saU6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(image_path):\n    ## applying tensorflow api to read img file , convert image jpeg file to array with reshape\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image,channels=3)\n    image = tf.image.resize(image,image_shape) \n    ## preprocess image i.e normalize in corect format for Inception V3\n    preprocessed_image = tf.keras.applications.inception_v3.preprocess_input(image)\n\n    return preprocessed_image,image_path","metadata":{"id":"OQluRsaSC7u6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## checking the function with one image \npreprocessed_image = preprocess_image(all_img_vector[0])\nprint(\"Image Shape :\", preprocessed_image[0].shape)\nprint(\"\\n\")\nprint(\"Image Vector Values after normalize :\\n\\n\",preprocessed_image)\nprint(\"\\n Display Image after processing:\\n\")\nplt.imshow(preprocess_image(all_img_vector[0])[0])","metadata":{"id":"YI5CKW1-uQX2","outputId":"5b5d2818-ef97-48a6-ff29-d17696def22c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the train & test data \n1.Combine both images & captions to create the train & test dataset using tf.data.Dataset API. Create the train-test spliit using 80-20 ratio & random state = 42\n\n2.Make sure you have done Shuffle and batch while building the dataset\n\n3.The shape of each image in the dataset after building should be (batch_size, 299, 299, 3)\n\n4.The shape of each caption in the dataset after building should be(batch_size, max_len)\n","metadata":{"id":"Dtcb4Y5CC7u7"}},{"cell_type":"code","source":"BATCH_SIZE = 512","metadata":{"id":"pRA4sUVllSqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Image dataset of preprocessed images into batches\nimages_path = sorted(set(all_img_vector)) ## taking only the unique paths\nimage_dataset = tf.data.Dataset.from_tensor_slices(images_path)\nimage_dataset = image_dataset.map(preprocess_image , num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)","metadata":{"id":"sbkhDIf-C7u7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### creating train and test split of image paths and caption vectors\nimage_train, image_test, caption_train, caption_test = train_test_split(df.Path,\n                                                                        cap_vector,\n                                                                        test_size=0.2,\n                                                                        random_state=42)","metadata":{"id":"-d7qDho8gxMh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## checking size of each train and test sets\nprint(\"Training data for images: \" + str(len(image_train)))\nprint(\"Testing data for images: \" + str(len(image_test)))\nprint(\"Training data for Captions: \" + str(len(caption_train)))\nprint(\"Testing data for Captions: \" + str(len(caption_test)))","metadata":{"id":"BExYkRLZXbBz","outputId":"283df0e4-e46f-4f02-903c-7ba3b570c61b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the pretrained Imagenet weights of Inception net V3\n\n1.To save the memory(RAM) from getting exhausted, extract the features of thei mage using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n\n2.The shape of the output of this layer is 8x8x2048. \n\n3.Use a function to extract the features of each image in the train & test dataset such that the shape of each image should be (batch_size, 8*8, 2048)\n\n","metadata":{"id":"grdhsiCkC7u7"}},{"cell_type":"code","source":"## building the transfer learning model to extract the features from the images\nimage_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n\n## input same shape as InceptionV3 and we have preprocessed the image vector accordingly\nnew_input = image_model.input\n\n## output same shape as InceptionV3 last dense layer , not the softmax layer as we dropped it while model selection above\nhidden_layer = image_model.layers[-1].output \n\nimage_features_extract_model = tf.keras.Model(inputs=new_input, outputs=hidden_layer)\nimage_features_extract_model.summary()","metadata":{"id":"AuUTQu8_C7u7","outputId":"9218061a-5361-4db4-ea35-8b127a53a166"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### filename for saving the img features after extract\nimg_feature_dict_filename = 'img_feature_dict.npy'","metadata":{"id":"mkoQ5B31ZFr8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### save and read the feature directory so that we don't have to rerun the model everytime.\n\ndef read_img_features(filename):\n  ##if file is not extracted the extarct and use it\n  read_dictionary = np.load(filename,allow_pickle='TRUE').item()\n  return read_dictionary\n\ndef save_img_features(filename,feature_dict):\n  np.save(filename, feature_dict)","metadata":{"id":"b13P1CK7xpvb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## extracting the feature vector from each image and saving it in dictionary\nimage_feature_dict = {}\n\n## if extarcted file is present or the zip is present then read from there else create feature file\nif os.path.isfile(img_feature_dict_filename):\n  ## extract the image features\n  image_feature_dict = read_img_features(img_feature_dict_filename)\n\nelse:\n  for image,path in tqdm(image_dataset):\n      ## extracting features via transfer learning model\n      batch_features = image_features_extract_model(image)\n      batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n      for batch_f, p in zip(batch_features, path):\n        path_of_feature = p.numpy().decode(\"utf-8\")\n        ## creating the dictionary via img path as id and feature vector as value\n        image_feature_dict[path_of_feature] =  batch_f.numpy()  \n\n  ## save the image features for reuse\n  save_img_features(img_feature_dict_filename,image_feature_dict)\n    ","metadata":{"id":"LLAZcrsxC7u8","outputId":"d883b096-5a92-431b-e08d-1767bc923d24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### function to get the img features and captions together\ndef map_function(image_name,caption):\n    image_tensor = image_feature_dict[image_name.decode('utf-8')]\n    return image_tensor,caption","metadata":{"id":"ZtvDSI0u7EJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### function to create a dataset having image feature vector and corresponding caption vector , using autotune and batch fetch\nBUFFER_SIZE = 3000\ndef generate_dataset(images_data, captions_data):\n    \n    dataset = tf.data.Dataset.from_tensor_slices((images_data, captions_data))\n    dataset = dataset.shuffle(BUFFER_SIZE) ## shuffling the dataset\n\n    ## applying the map function on the dataset to extract the image features from the previously created image_feature_dict via the img path value\n    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n          map_function, [item1, item2], [tf.float32, tf.int32]),\n          num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)\n\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE) ## appling prefetch based on Autotune buffer size for utilizing optimal resource\n    return dataset","metadata":{"id":"06rBdmhLZXeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### creating the test and train dataset after shuffle and batch\ntrain_dataset=generate_dataset(image_train,caption_train)\ntest_dataset=generate_dataset(image_test,caption_test)","metadata":{"id":"kpnrdzLRf1pU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## checking the shape of the sample batch for image and captions vector\nsample_img_batch, sample_cap_batch = next(iter(train_dataset))\nprint(sample_img_batch.shape)  #(batch_size, 8*8, 2048)\nprint(sample_cap_batch.shape) #(batch_size,40)","metadata":{"id":"jw8bwfvwC7u8","outputId":"372a1ff2-61ae-45c4-e236-17c7745118ea"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building\n1.Set the parameters\n\n2.Build the Encoder, Attention model & Decoder","metadata":{"id":"UNTf3gf9C7u8"}},{"cell_type":"code","source":"embedding_dim = 256 \nunits = 512\nvocab_size = 5001","metadata":{"id":"zeK2qPubC7u8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoder","metadata":{"id":"pIffD9qzC7u8"}},{"cell_type":"code","source":"## this will take the features vectors already created via Inception V3 model above and reduce the dimension as per embedding vector shape \n## so as to keep both feature and caption vector in same shape for Attention model\nclass Encoder(Model):\n    def __init__(self,embed_dim):\n        super(Encoder, self).__init__()\n        self.fc = layers.Dense(embed_dim , activation=\"relu\") ## applying relu activation on the fc layer\n        self.dropout = layers.Dropout(0.4) ## applying dropout on the fc layer to reduce overfitting\n        \n    def call(self, features):\n        features = self.fc(features)\n        return features","metadata":{"id":"8hM61FhPC7u8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## creating object of the Encoder model\nencoder=Encoder(embedding_dim)","metadata":{"id":"B0xR0YLHC7u9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Attention model","metadata":{"id":"kMBIxIFBC7u9"}},{"cell_type":"code","source":"## this custom model will take the feature vector from encoding model and \n## hidden vector from the decoding RNN model and create the context vector based on attention weights calculated \n\nclass Attention_model(Model):\n    def __init__(self, units):\n        super(Attention_model, self).__init__()\n        self.W1 = layers.Dense(units) ## for Img Feature Vector from Encoder\n        self.W2 = layers.Dense(units) ## for Caption Hidden Vector from Decoder\n        self.V = layers.Dense(1) ## for Scoring based on Feature & Hidden Vector\n        self.units=units\n\n    def call(self, features, hidden):\n        #features shape: (batch_size, 8*8, embedding_dim)\n        # hidden shape: (batch_size, hidden_size)\n        hidden_with_time_axis =  tf.expand_dims(hidden, 1) ## adding an extra dimention to Hidden Decoder vector to match feature vector dimension\n        score = keras.activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis)) ## calculating the score vector from Feature and Hidden Vector\n        attention_weights =  keras.activations.softmax(self.V(score), axis=1) ## converting the Score vector into attention weights via Softmax layer\n        context_vector = attention_weights * features ## creating the context vector from Feature vector\n        context_vector = tf.reduce_sum(context_vector, axis=1) \n        return context_vector, attention_weights","metadata":{"id":"pXWve33VC7u9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decoder","metadata":{"id":"a6SwmF_8C7u9"}},{"cell_type":"code","source":"class Decoder(Model):\n    def __init__(self, embed_dim, units, vocab_size):\n        super(Decoder, self).__init__()\n        self.units=units ## setting the decoder units\n        self.attention = Attention_model(self.units) ## setting the attanetion model units for creating the W1 & W2 dense layers\n        self.embed = layers.Embedding(vocab_size, embed_dim,mask_zero=False) ## creating an embeding layer for converting the caption vector as per embed dimension which matches the encoder output layer\n        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform') ## defining the RNN GRU for predicting the words sequencially\n        self.d1 = layers.Dense(self.units) ## defining a dense layer as per the decoder units\n        self.d2 = layers.Dense(vocab_size) ## defining a dense layer for final decoder output  \n        self.dropout = layers.Dropout(0.4) ## defining a dropout on the fc layer to reduce overfitting\n        \n\n    def call(self,x,features, hidden):\n        context_vector, attention_weights = self.attention(features, hidden) ## calling the attention model to provide the context vector\n        embed = self.dropout(self.embed(x)) ## applying the dropout defined above on the embedding layer\n        mask = self.embed.compute_mask(x) ## applying masking on the  embedded input caption vector to not consider the padded values for model traning\n        embed =  tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1) ## concatting the captions embedded masked vector with the context vector\n        output,state = self.gru(embed,mask=mask) ## passing the final embedded vector after concat into the GRU\n        output = self.d1(output) ##passing the output of GRU via the dense layer 1 defined above\n        output = tf.reshape(output, (-1, output.shape[2])) ## reshaping the output layer of dense layer 1\n        output = self.d2(output) ## passing the final output via the last dense layer 2 for match the output vector dimension\n        return output,state, attention_weights\n    \n    def init_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","metadata":{"id":"ADEyD3EqC7u9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## creating the decoder object\ndecoder=Decoder(embedding_dim, units, vocab_size)","metadata":{"id":"dXQqscjLC7u-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## checking the different model output shape from sample train batch\nfeatures=encoder(sample_img_batch) ## creating the encoder model\n\nhidden = decoder.init_state(batch_size=sample_cap_batch.shape[0]) ## creating the initial hidden layer\ndec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1) ## creating the decoder input\n\npredictions, hidden_out, attention_weights= decoder(dec_input, features, hidden) ## creating the decoder model\nprint('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\nprint('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\nprint('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)","metadata":{"id":"wNE2Xo5aC7u-","outputId":"6abf06b5-5ead-4240-bd1b-cf993bfbccf2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model training & optimization\n1.Set the optimizer & loss object\n\n2.Create your checkpoint path\n\n3.Create your training & testing step functions\n\n4.Create your loss function for the test dataset","metadata":{"id":"JEKgDUjQC7u-"}},{"cell_type":"code","source":"## defining the optimizer and loss objects\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n##optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n\nloss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')","metadata":{"id":"ucRim8sNC7u-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## creating the custom loss function which handles the padded values while calculating the mean loss\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0)) ## creating the mask because we don't want to consider captions padding values for loss function else it will be wrong\n    \n    loss_ = loss_object(real, pred) ## calculating the loss value via the keras.losses.SparseCategoricalCrossentropy\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask ## applying the masked value on the top of loss output so that we don't consider padded value's loss\n\n    return tf.reduce_mean(loss_) ## returning the mean loss value","metadata":{"id":"_RkJLlHPC7u-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## defining the checkpoint object to save the group of trackable objects to a checkpoint file during model training\ncheckpoint_path = \"/content/checkpoints/\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)","metadata":{"id":"vziS4GKyC7u-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## if model training breaks in between, when we start it again then it will check for last checkpoint \n## and start the epoch from there else will start from 0\nstart_epoch = 0\n\nif ckpt_manager.latest_checkpoint:\n    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])","metadata":{"id":"23GVWCkbC7u-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## using tensorflow functions decorator in order to turn plain Python code into graph for faster computation\n\n@tf.function\ndef train_step(img_tensor, target): ##defining the model training function\n    loss = 0 ## setting the initail loss to 0\n    hidden = decoder.init_state(batch_size=target.shape[0]) ## setting up the initial hidden layer\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1) ## expanding the initial decorator input dimesnion by 1 from caption vector\n    \n    ## using gradiant tape for handling custom gradiant decent to update weights on the models\n    with tf.GradientTape() as tape:\n        features = encoder(img_tensor) ## extracting feature vector via the encoder\n        for i in range(1, target.shape[1]): ## running loop though each word in he caption vector\n            predictions, hidden, _ = decoder(dec_input, features, hidden) ## getting the output from the decoder\n            loss += loss_function(target[:, i], predictions) ## calculating the loss from the decoder output and actual target word\n            dec_input = tf.expand_dims(target[:, i], 1) ## decoder input with the actual word\n        avg_loss = (loss/int(target.shape[1])) ## calculating the average loss after all words are passed\n    trainable_variables = encoder.trainable_variables + decoder.trainable_variables ## finding all the trainable variables from the excoder and decoder models\n    gradients = tape.gradient(loss, trainable_variables) ## calculating the derivative of the total loss by all the trainable variables\n    optimizer.apply_gradients(zip(gradients, trainable_variables)) ## applying the Adams optimizer based on the trainable variables based on the gradiant calculated\n    return loss, avg_loss","metadata":{"id":"J_el6mxfC7u_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## using tensorflow functions decorator in order to turn plain Python code into graph for faster computation\n\n@tf.function\ndef test_step(img_tensor, target):##defining the model testing function\n    each_batch_loss = 0\n    hidden = decoder.init_state(batch_size=target.shape[0]) ## setting up the initial hidden layer\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1) ## expanding the initial decorator input dimesnion by 1 from caption vector\n\n    features = encoder(img_tensor) ## extracting feature vector via the encoder\n\n    ## iterating through each actual word and calculating the loss from prediction\n    for i in range(1, target.shape[1]):\n      predictions, hidden, _ = decoder(dec_input, features, hidden)\n      each_batch_loss += loss_function(target[:, i], predictions) ## calculating the loss by comparing with actual words\n      dec_input = tf.expand_dims(target[:, i], 1) ## decoder input with actual word\n\n    avg_batch_loss = (each_batch_loss / int(target.shape[1])) ## mean loss of each word predicted\n\n    return each_batch_loss, avg_batch_loss","metadata":{"id":"zAgUjGvDC7u_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## function to calculate the average test data loss\ndef test_loss_cal(test_dataset):\n    total_avg_batch_loss = 0\n    test_num_steps = 0\n    for (batch, (img_tensor, target)) in enumerate(test_dataset):\n        each_batch_loss, avg_batch_loss = test_step(img_tensor, target) ## finding the batch lossfrom test dataset\n        total_avg_batch_loss += avg_batch_loss\n        test_num_steps = test_num_steps + 1\n    avg_test_loss=total_avg_batch_loss/test_num_steps ## calculating he avg test loss from all the batches\n    return avg_test_loss","metadata":{"id":"uXBE-ornC7u_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Array to plot the train and test loss \nloss_plot = []\ntest_loss_plot = []","metadata":{"id":"n8CDIadcSRVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## executing the model training process , calcuting the training loss and test dataset loss on the model\n\nEPOCHS = 200 ## max epoch\nstop_epoch_count = 0\n\nbest_test_loss=100\n\nfor epoch in tqdm(range(0, EPOCHS)):\n    start = time.time()\n    total_avg_batch_loss = 0\n    train_num_steps = 0\n    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n        total_loss, avg_loss = train_step(img_tensor, target) ## finding the total and avg batch loss of train data\n        total_avg_batch_loss += avg_loss ## summing up the average loss\n        train_num_steps = train_num_steps + 1\n\n    avg_train_loss=total_avg_batch_loss / train_num_steps ##calculating the average of the total no of batches\n    loss_plot.append(avg_train_loss) ## appending the traning loss to plot graph   \n    test_loss = test_loss_cal(test_dataset) ## calculating the loss from test data\n    test_loss_plot.append(test_loss) ## appending the testing loss to plot graph \n    \n    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n    \n    ## for each epoch is the test data loss is less than previous then saving the model via checkpoint\n    if round(float(test_loss),3) < best_test_loss :\n      print('Test loss has been reduced from %.3f to %.3f thus saving the checkpoint \\n\\n' % (best_test_loss, test_loss))\n      best_test_loss = round(float(test_loss),3)\n      stop_epoch_count = 0 ## reset counter once checkpoint saved\n      ## save the group of trackable objects to a checkpoint file.\n      ckpt_manager.save()\n    else:\n      stop_epoch_count = stop_epoch_count + 1 ##increase counter for non improve test loss epoch\n      print(\"No Test Loss improvement in this Epoch\\n\")\n      if stop_epoch_count >= 5: ## if loss doesn't reduce in last 5 epochs then stop execution\n        print(\"No Improvement in Test Loss , hence we have reached the Global Minima. Stopping Epoch run.\")\n        break ","metadata":{"id":"3bXQ3lZzC7u_","outputId":"02c9b09c-24fc-41f7-baa9-5a9e754ec72f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## plotting the traning and testing dataset avg loss values\nplt.figure(figsize=(8,6))\nplt.plot(loss_plot,label='Traning Loss')\nplt.plot(test_loss_plot,label='Testing Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.legend()\nplt.show()","metadata":{"id":"fKt_nDjgC7u_","outputId":"da3d7514-f4a5-4d2c-b521-77cce237d476"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation\n1.Define your evaluation function using greedy search\n\n2.Define your evaluation function using beam search ( optional)\n\n3.Test it on a sample data using BLEU score","metadata":{"id":"_OZn93E-C7u_"}},{"cell_type":"markdown","source":"### Greedy Search","metadata":{"id":"GHki7xMfC7vA"}},{"cell_type":"code","source":"def evaluate(image):\n    max_length=39\n\n    hidden = decoder.init_state(batch_size=1) ## creating the hiden state for 1 item for Decoder model\n\n    temp_input = tf.expand_dims(preprocess_image(image)[0], 0) #process the input image to desired format before extracting features\n    img_tensor_val = image_features_extract_model(temp_input) ## extracting the features from img via Inception V3 transfer learning\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3])) ## reshapring the image\n\n    attention_plot = np.zeros((max_length, img_tensor_val.shape[1])) ## creating the attention feature vector based on the caption max lenth\n\n    features = encoder(img_tensor_val) ## passing the feature vector via encoder model\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0) ## creating the initial Decoder input\n    result = []\n\n    for i in range(max_length): ## running the loop though the max word count of the caption vector\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden) ## predicting each word via the Decoder model\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy() ## adding the addention weights for the word returned from Decoder into attention_plot\n\n        predicted_id = tf.argmax(predictions[0]).numpy() ## extracting the max probability word token from predictions vector\n        result.append(tokenizer.index_word[predicted_id]) ## converting the token to word and saving it into list\n\n        if tokenizer.index_word[predicted_id] == '<end>': ## if reach end of line via end tag then return and exit\n            return result, attention_plot,predictions\n\n        dec_input = tf.expand_dims([predicted_id], 0) ## else Decoder input the next word\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot,predictions","metadata":{"id":"wu91c88ZC7vA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Beam Search","metadata":{"id":"1tSqwpJaKnLd"}},{"cell_type":"code","source":"def beam_evaluate(image, beam_index = 3):\n    max_length=39\n    start = [tokenizer.word_index['<start>']]\n    result = [[start, 0.0]]\n\n    hidden = decoder.init_state(batch_size=1)\n\n    temp_input = tf.expand_dims(preprocess_image(image)[0], 0) #process the input image to desired format before extracting features\n    img_tensor_val = image_features_extract_model(temp_input) ## extracting the features from img via Inception V3 transfer learning\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3])) ## reshapring the image\n\n    attention_plot = np.zeros((max_length, img_tensor_val.shape[1])) ## creating the attention feature vector based on the caption max lenth\n\n    features = encoder(img_tensor_val) ## passing the feature vector via encoder model\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0) ## creating the initial Decoder input\n\n    while len(result[0][0]) < max_length: ## running a loop though the max number of words\n        i=0\n        temp = []\n        for s in result: ## running loop through each word predicted and stored in results list\n            predictions, hidden, attention_weights = decoder(dec_input, features, hidden) ## preduct the words from decoder along with attanetion wts and hidden layer\n            attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy() ## adding the addention weights for the word returned from Decoder into attention_plot\n            i=i+1\n            word_preds = np.argsort(predictions[0])[-beam_index:] ## selecting the max probality words by sorting based on beam index provided as input i.e top 3\n          \n            for w in word_preds: ## running a loop through the top beam index words from predictions vector returned above\n                next_cap, prob = s[0][:], s[1] ## fetching the word and probability of the next word\n                next_cap.append(w) \n                prob += np.log(predictions[0][w]) ## calculating the cumulative sum of probabilities of the top words predicted and appeneded \n                temp.append([next_cap, prob]) ##saving the combilation and total probaility in a variable\n        result = temp ## once out of aloop sav ethe temp into results\n        result = sorted(result, reverse=False, key=lambda l: l[1]) ## sort the results array in reverse order to get the max probability combination on top\n        result = result[-beam_index:] ##selecting gthe top beam index combinations\n        \n        \n        predicted_id = result[-1]\n        pred_list = predicted_id[0]\n        \n        prd_id = pred_list[-1] \n        if(prd_id!=3): ## break if reach <end> tag\n            dec_input = tf.expand_dims([prd_id], 0)  ## providing the best combination as input to the Decoder in next run\n        else:\n            break\n    \n    \n    result2 = result[-1][0]\n    \n    ## creating the setence from the caption vector values\n    intermediate_caption = [tokenizer.index_word[i] for i in result2]\n    final_caption = []\n    for i in intermediate_caption:\n        if i != '<end>':\n            final_caption.append(i)\n            \n        else:\n            break\n\n    attention_plot = attention_plot[:len(result), :]\n    final_caption = ' '.join(final_caption[1:])\n    return final_caption ,attention_plot","metadata":{"id":"PUQLxlVBKdDD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_attmap(caption, weights, image):\n\n    fig = plt.figure(figsize=(10, 10))\n    temp_img = np.array(Image.open(image)) ## extracting the image array\n    \n    len_cap = len(caption) ## getting the max lenth of the caption\n    for cap in range(len_cap): ## looping through the max word count and displaying the focused area of the image from where the word is predicted\n        weights_img = np.reshape(weights[cap], (8,8)) ## getting the attention weight vector on each word\n        weights_img = np.array(Image.fromarray(weights_img).resize(image_shape, Image.LANCZOS)) ## reshaping the weight vector\n        \n        ax = fig.add_subplot(len_cap//2, len_cap//2, cap+1) ## calculating the number of subplots based on no of words\n        ax.set_title(caption[cap], fontsize=15) ## displaying the word predicted from each part of the focussed image\n        \n        img=ax.imshow(temp_img) ## displaying the actual image\n        \n        ## putting an overlay on top of the actual image to focus on a particular area based the attention weights for that word\n        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent()) \n\n        ax.axis('off')\n    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n    plt.show()","metadata":{"id":"c-brdjyFGdwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu","metadata":{"id":"p_xXSz6UGfFi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## fuction to remove start end and unk tags from the captions predicted\ndef filt_text(text):\n    filt=['<start>','UNK','<end>'] \n    temp= text.split()\n    [temp.remove(j) for k in filt for j in temp if k==j]\n    text=' '.join(temp)\n    return text","metadata":{"id":"0dPig2JiGiZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0,len(df)) ## selecting a random index from dataframe\nprint(\"Random Test Image ID :\" , rid)\ntest_image = df.Path[rid]\n\n## getting the real caption from the df\nreal_captions = df.Captions[df.Path == test_image]\n\n## extarting the prediction words from the image along with attention weight vctor for each word from Greedy search\ngreedy_result, greedy_attention_plot,pred_test = evaluate(test_image)\n\n## extarting the prediction words from the image along with attention weight vector for each word from Beam search\nbeam_result, beam_attention_plot = beam_evaluate(test_image)\n\n## creating the prediction sentence\ngreedy_pred_caption=' '.join(greedy_result).rsplit(' ', 1)[0]\ngreedy_candidate = greedy_pred_caption.split() ## predicted word list from greedy search\nbeam_candidate = beam_result.split() ## predicted word list beam search\n\nfiltered_real_captions = [] ## this is used to display with the actual image\n\ngreedy_bleu_score = 0\nbeam_bleu_score = 0\n\nfor real_caption in real_captions :\n  ## removing the start , end and UNK from the real caption sentence\n  real_caption=filt_text(real_caption)  \n\n  ## adding the filtered the sentence to the list for future display\n  filtered_real_captions.append(real_caption) \n\n  ## extracting the words from the real caption sentence\n  reference = [] \n  reference.append(real_caption.split())\n\n  ## getting the BLEU score for Greedy search words from the predicted and actual words for a image for each real caption\n  greedy_bleu_score = greedy_bleu_score + sentence_bleu(reference, greedy_candidate, weights=(0.25, 0.25, 0.25, 0.25))  \n\n  ## getting the BLEU score for Beam search words from the predicted and actual words for a image for each real caption\n  beam_bleu_score = beam_bleu_score + sentence_bleu(reference, beam_candidate, weights=(0.25, 0.25, 0.25, 0.25)) \n\n## showing the average BLEU score comparing with all the actual captions for the image\nprint(\"\\nMean BELU score for Greedy Search: \" , round(((greedy_bleu_score / len(real_captions))*100),3)) \nprint(\"\\nMean BELU score for Beam Search: \" , round(((beam_bleu_score / len(real_captions))*100),3)) \n\nprint('\\nGreedy Search Prediction Caption:', greedy_pred_caption)\nprint('\\nBeam Search Prediction Caption:', beam_result)\n\n## displying the attantion plot map to understand how each is predicted by focusing on each part of image -- Greedy Search\nprint('\\nGreedy Search Plot:')\nplot_attmap(greedy_result, greedy_attention_plot, test_image)\n\nprint('\\nReal Captions:')\nfor caption in filtered_real_captions :\n  print(caption)\n\n## displaying actual image\nImage.open(test_image)","metadata":{"id":"JqYz_-DxGob4","outputId":"c99be27c-ddc8-4b40-ee0d-73c6a817bb59"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Installing and Importing the Google Text to Speech API","metadata":{"id":"1f3dy7cFY2lb"}},{"cell_type":"code","source":"! pip install gTTS","metadata":{"id":"4aA-4OItY83a","outputId":"9d331f4d-f8d1-4c0b-c1fb-d3197c7b7014"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the required module for text to speech conversion\nfrom gtts import gTTS","metadata":{"id":"2hxYgHR3ZAD8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Function to predict Text & Audio from any input image","metadata":{"id":"8JzMXCMTe4e1"}},{"cell_type":"code","source":"def EyeForBlind(imagepath):\n  result, __,_ = evaluate(imagepath) ## using Greedy Search\n  beam_pred ,_ = beam_evaluate(imagepath) ## using Beam Search\n\n  ## creating the prediction sentence\n  pred_caption=' '.join(result).rsplit(' ', 1)[0]\n\n  print('\\nImage to Text Caption via Greedy Search :', pred_caption)\n  print('\\nImage to Text Caption via Beam Search :', beam_pred)\n  print('\\n')\n\n  ## display the test image\n  plt.figure(figsize=(10,8))\n  plt.imshow(mpimg.imread(imagepath))\n\n  # Language in which you want to convert\n  language = 'en'\n  \n  # getting the sound from gTTS by passing the predicted caption\n  soundobj = gTTS(text=pred_caption, lang=language, slow=False)\n  \n  # Saving the converted audio in a mp3 file named\n  soundobj.save(\"Image_to_Sound.mp3\")\n\n  # Playing the converted file\n  os.system(\"./Image_to_Sound.mp3\")\n  ","metadata":{"id":"lafh8E0Fe6RW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## checking the model prediction with random uploaded image from google\nEyeForBlind(\"test.jpg\")","metadata":{"id":"6Wy_sgwrhOQ0","outputId":"40cf0814-f489-4629-fc69-de39e9b7ac2c"},"execution_count":null,"outputs":[]}]}